---
title: TinyLFU
description: 空间消耗少，近似精确的LFU算法
toc: true
authors: ["李冬瓜"]
tags: ["缓存","服务端"]
categories: ["缓存"]
series: []
date: 2023-09-13T20:07:16+08:00
lastmod: 2023-09-13T20:07:16+08:00
featuredVideo:
featuredImage:
draft: false
---

## 1.背景：
### 1.1存在背景
缓存存在原因：程序的执行过程：首先从硬盘执行程序，存放到内存，再给cpu运算与执行。由于内存和硬盘的速度相比cpu速度差距大，每执行一个程序cpu都要等待内存和硬盘，引入缓存技术便是为了解决此矛盾，缓存与cpu速度一致，cpu从缓存读取数据比cpu在内存上读取快得多，从而提升系统性能。  

缓存淘汰策略存在的原因：缓存的关注点之一，是在于如何提升缓存的命中率，而缓存的淘汰策略模型就是为了预测哪些数据以后可能会被访问到，以达到更高的命中率、更好的性能。  

有不同的缓存淘汰策略的原因：由于不同系统的数据访问模式不同，同一种缓存策略很难在不同的数据访问模式下取得满意的性能。如下是几种缓存策略的分类：
- 基于访问的时间：此类算法按各缓存项被访问时间来组织缓存队列，决定替换对象。如LRU
- 基于访问频率：此类算法用缓存项的被访问频率来组织缓存。如LFU、LRU2、2Q、LIRS。
- 访问时间与频率兼顾：通过兼顾访问时间和频率。使得数据模式在变化时缓存策略仍有较好性能。如FBR、LRUF、ALRFU。多数此类算法具有一个可调或自适应参数，通过该参数的调节使缓存策略在基于访问时间与频率间取得一个平衡。
- 基于访问模式：某些应用有较明确的数据访问特点，进而产生与其相适应的缓存策略。如专用的VoD系统设计的A&L缓存策略，同时适应随机、顺序两种访问模式的SARC策略。

### 1.2横向对比
缓存策略中主要有两种：LRU(Least Recently Used)、LFU(Least Frequently Used)，使用LRU或使用LFU就对应的引出了两个问题：  

缓存污染问题：由于偶发性或周期性的冷数据批量查询，热点数据被挤出去，导致缓存命中率下降，影响缓存整体效率。  

访问模式问题：一旦访问模式改变，缓存需要更长时间来适应新的访问模式，导致缓存命中率下降，进而拉低整体效率。

|    | LRU                                                              | LFU                                                           |
|  ----  |------------------------------------------------------------------|---------------------------------------------------------------|
| 实现方式 | 记录访问顺序，使用时间最远的数据先被移除。![img.png](/pic/tinyLFU/lru.png)            | 记录访问次数，使用次数最少的数据先被移除。![img.png](/pic/tinyLFU/lfu.png)         |
| 应用场景 | 能够迅速反映随时间变化的数据访问模式，在热点数据场景下很适用。                                  | 数据访问模式固定时，如：CDN缓存（将大多数用户可能请求的静态文件放入缓存中）                       |
| 优点   | 能够解决访问模式问题，优先保证热点数据有效性，常规场景下有着不错的命中率。                            | 能够解决缓存污染问题，当数据访问模式固定时，通过统计访问频次能够带来最佳的缓存命中率。                   |
|  缺点  | 存在缓存污染问题，认为最后到来的数据是最可能被访问到的。例：循环访问所有数据(数据量>缓存)，且每次循环只访问一次，命中率降低。 | 存在访问模式问题，需要频繁更新数据块计数以及排序，代价昂贵。例：过去访问次数多但最近不访问数据 会占用缓存，命中率会降低。 |  



|    | TinyLFU                                                                                 | LFU                              | Aging-LFU                                                                          |
|  ----  |-----------------------------------------------------------------------------------------|----------------------------------|------------------------------------------------------------------------------------|
|   实现     | 见深入TinyLFU原理                                                                            | 根据数据的历史访问频率来淘汰数据，淘汰一定时间内最少使用的数据。 | 基于LFU，增加一个引用计数，如果当前缓存中的数据“引用计数平均值”>=“引用计数最大平均值”时，则将所有数据的引用计数减少为原来的一半或减去固定的值(老化机制)。 |
|  命中率      | 高；新鲜机制缓解访问模式问题                                                                          | 低；存在访问模式问题                       | 中；降低“访问模式”的问题(缩小为原来的一半)。                                                           |
|  复杂度      | 低；空间复杂度 O(d*m)；Sketch、Doorkeeper消耗空间；时间复杂度：查询频率： O(1)、插入：O(k)，k为哈希函数个数即为d、Reset：均摊为O(k) | 中；需要维护一个访问历史队列，每个数据维护引用计数。       | 高；除维护访问历史队列之外，增加了平均引用次数的判断、处理。                                                     |
|  代价      |   低，空间消耗小，不必统计所有元素出现的确切次数。时间消耗少，通过哈希函数映射位置的值，估计出元素出现的次数。                                                                                      | 高，需要记录所有数据的访问记录，需要基于引用计数排序。      | 高，基于LFU，增加的老化机制。                                                                   |


## 2.TinyLFU介绍
论文名字：TinyLFU: A Highly Efficient Cache Admission Policy；    

作者：GIL EINZIGER, Nokia Bell Labs ROY FRIEDMAN, Technion BEN MANES, Independent

### 2.1 概念：
![img.png](/pic/tinyLFU/tinyLFU_overview.png)

TinyLFU是一种基于LFU，并且专门为了解决 LFU 两个问题(访问模式、计数与排序代价昂贵)而被设计出来的缓存准入策略。TinyLFU让记录尽量保持相对的“新鲜”（Freshness Mechanism），
降低访问模式变化时对缓存命中率的影响，同时减少对应的空间、时间消耗。TinyLFU被用在开源缓存框架Caffeine中，被称为现代的缓存。


|  名词  | 简单解释                                                   |
|  ----  |--------------------------------------------------------|
|   Bloom Filter 布隆过滤器(BF)     | 一种通过极少空间近似判断元素是否已经于集合中存在的算法。                           |
|  Counting Bloom Filter (CBF)      | 一种通过极少空间近似判断元素在集合中出现次数的算法。                             |
| Count-Min Sketch       | 一种估计元素出现次数的算法，在于CBF的基础之上，进一步防止对大型计数器进行不必要的递增，并产生更好的估计。 |
| Doorkeeper机制       | 拦截在CM-Sketch之前的一个布隆过滤器，能够避免为长尾流量分配计数器                  |
| 长尾流量 heavy-tailed       | ![img.png](/pic/tinyLFU/heavy_tailed.png)              |
| SLRU       | 基于LRU的缓存淘汰策略算法，能够降低缓存污染问题的影响。                                                       |
|Caffeine         |  https://github.com/ben-manes/caffeine/issues/504                                                       |

### 2.2特点
- 额外空间小。
- 访问模式变化时依旧具有良好表现。

#### 2.2.1 额外空间小-CountMin Sketch & Doorkeeper
>思考：为什么TinyLFU极大减少了内存消耗？

CountMin Sketch：极大降低统计出现次数与排序成本，在此数据结构中只需要知道缓存中已经存在的潜在缓存替换受害者(victim)是否比当前正在访问的项目更受欢迎。但是，无需确定高速缓存中所有元素之间的确切顺序。  

计数器位数少：通过降低Sketch计数器的位数，降低单个计数器使用空间成本，进而降低总消耗空间。  

使用Doorkeeper过滤长尾流量：防止为长尾流量分配计数器，导致缓存污染问题出现。  

#### 2.2.2 解决访问模式问题-新鲜机制(Freshness Mechanism)
>思考：基于统计次数的缓存淘汰策略算法如何保持新鲜机制？

使用全局计数器记录访问次数：当访问量达到一定值时，将所有计数器的值除以2，降低过去对现在的影响，但保持一定的过去对现在的参考性。  

## 3.深入TinyLFU原理

### 3.1.Counting Min Sketch
#### 3.1.1  Bloom Filter(前置知识)
1. 概念：布隆过滤器(bloom filter)是空间效率很高的一种概率数据结构，用于判断一个元素是否在一个集合中。Bloom filter 判断一个元素不存在集合中，则肯定不存在，如果判断一个元素存在于集合中，有一定的概率判断错误。有一定的判断误差，因此适合容忍低错误率的场景。
2. 实现原理：每次想要插入或查询一个元素是否在集合中时，只需要使用k个哈希函数对元素求值，并将对应的比特位标记或检查对应的比特位即可。
![img.png](/pic/tinyLFU/bloom_filter_hash.png)
3. 优点：相比于哈希表、链表等数据结构，其空间和时间的优势明显。而且布隆过滤器的插入、查询时间都是常数O(k)。
4. 缺点：其返回的结果是概率性的，而不是确切的，且无法从布隆过滤器中删除元素。
> 思考：为什么BF不支持删除操作？ 
> 1. 需要确保元素存在集合中(误判，有可能本来就没有，也就不支持删除)
> 2. 需要保证不会对其他元素的存在造成影响(多个元素经过hash函数映射到同一位bit)
5. 应用场景：1.如为每一个用户推送新闻时，需要判断此新闻是否之前给用户推送过，如果布隆过滤器显示不存在则一定没有推送过，代表可以推送给用户。如果通过布隆过滤器显示已经推送过，虽然不能够确定，但是这对新闻量很大的应用场景来说，是否推送这条新闻并不会有很大影响。2.网页爬虫对URL的去重，避免爬取相同的URL地址。
#### 3.1.2 Counting Bloom Filter(前置知识)
1. 概念：基于布隆过滤器的基础上，能够支持元素删除的操作，也能够对元素出现的次数进行估计。
2. 实现原理：使用一个合适大小的计数器代替了BF中1bit，在插入时 对hash得到的索引位置的所有值进行+1操作。在读取时 通过hash得到不同索引位置，记录索引位置对应最小值即为元素估计出现次数。在删除时，首先判断各哈希位置值是否都大于0，如果都大于0，则对hash得到的所有索引位置值进行-1操作。
![img.png](/pic/tinyLFU/counting_bloom_filter.png)
3. 优点：对于插入、查询操作，时间、空间复杂度相比Map，List等数据结构优势明显。
4. 缺点：其返回的结果是概率性的，而不是确切的。如果可被删除，则只能用来做对象的估计，而 「不存在则一定不存在」的特性并不具备。
5. 使用场景：近似估计某一元素出现的次数,比如：统计某网站中某网页的日活DAU。

#### 3.1.3 Sketch And Count-Min Sketch
1. 概念：Sketch和Count-Min Sketch  能够使用较小的空间勾勒出数据集内各类事件的频次。比如，我们可以统计出当前最热门的推特内容，或是计算网站访问量最大的页面。当然，这一算法同样会牺牲一定的准确性。
2. 实现原理：
- 1.选定d个hash函数，创建一个 d x m 的二维整数数组作为哈希表。这里d,m的取值根据delta有关(误差概率要求)。
- 2.Sketch：对每个元素，分别使用d个hash函数计算相应的哈希值，并分别对m取余，第i个哈希函数映射的位置 在表(m,d)中表现为 ( hash_res % m , i )，然后在对应的位置上增1。
- 3.Count-Min Sketch：与Sketch唯一不同的是，Count-Min Sketch在更新时，读取所有k个计数器，并且仅递增最小计数器(如果有多个最小计数器，则全部递增)。例如：新元素插入时将读取d个计数器。假设d=3,我们读得值为2、2、5，则“加”操作仅将剩下的两个计数器从2递增到3，而第三个计数器保持不变。
- 4.要查询某个元素的频率时，只需要取出d个值, 返回最小的那一个。
- 如下图1为Sketch实现：
- 如下图2为Count-Min Sketch实现  

Sketch:
![img.png](/pic/tinyLFU/sketch.png)

Count-Min Sketch:
![img.png](/pic/tinyLFU/count_min_sketch.png)

3. Sketch｜Count-Min Sketch共同优点：节省内存，存储元素出现的次数，而不存储元素本身。
4. Count-Min Sketch相比Sketch优点：此加法操作可防止对大型计数器进行不必要的递增，并针对高频项产生更好的估计，因为大多数低频项不太可能使它们的计数器递增。它不支持减量，但是可以减少高频计数的误差。
5. 缺点：对于出现次数比较少的元素，准确性很差，主要是因为hash冲突比较严重，产生了噪音，例如当w=20时，有1000个数hash到这个20桶，平均每个桶会收到50个数，这50个数的频率重叠在一块了。
6. 使用场景：在TinyLFU中，采用Count-Min Sketch算法近似估计｜记录元素出现的次数。
>思考：为什么Sketch是二维分布(d行w列)，而计数布隆过滤器是一维数组？两者的区别是什么？为什么Sketch要这样做？   
> 
> 普遍认为CountMin Sketch较优，哈希冲突只能通过 同一哈希函数 哈希不同元素得到相同位置 进行碰撞，避免了不同哈希函数之间的冲突，从而降低误差，具体请看如下链接  
> https://dl.acm.org/doi/abs/10.1145/1007568.1007588

### 3.2 新鲜机制 Freshness Mechanism
1. 概念：每次我们向Count-Min Sketch添加一个元素时，我们都会让一个全局计数器S增加1。一旦此计数器S的值达到样本大小(W)，就将S计数器的值和Sketch中所有计数器近似值除以2。
2. 优点：
- 此机制并不需要太多额外的空间，因为它唯一增加的存储成本是单个全局计数器S。
- 此方法提高了高频项目的准确性。此重置方法可以提高Sketch精度，而不会增加总空间成本。
- 在固定的频率分布下完全正确，且可以应对流量频率的变化
3. 缺点：
- 每次对全局计数器S和其他所有计数器除2时，如果其数值为奇数，则除完以后会有截断误差(5/2=2)，不过当数据样本越大，此截断误差的影响就越小。
- 每次当计数器S到达样本大小时，需要对所有计数器进行除法。然而，可以使用移位寄存器在硬件中有效地实现此操作，其摊销复杂度是常数，使其对于许多应用程序都是可行的。  
4. 作用：相比LFU ，此机制使得TinyLFU较好地应对访问模式变化所带来的影响(老化机制，随着时间的流逝，过去的出现影响逐渐变小)。
>探索：为什么保持新鲜机制是除以2而不是除以其他？这种机制的正确性？  
> 论文中3.3小节给出了具体的数学推导证明  

### 3.3 Space Reduction 空间减少
>思考：如何能够减少空间消耗？  
> 从两个维度来看：   
> 1. 减少每一个计数器的位数 
> 2. 减少计数器的数量

#### 3.3.1  Small Counters
>减少计数器位数  

1. 背景：单纯地实现Count-Min Sketch需要使用长计数器。如果Sketch包含W个唯一请求，则需要允许计数器计数到W，从而导致每个计数器都为Log(W)位，所消耗空间较多。比如：有16k个请求，则每一个计数器的位数应该是14位。
2. 改进方案：只需要知道缓存中已经存在的潜在缓存替换受害者(victim)是否比当前正在访问的项目更受欢迎。但是，无需确定高速缓存中所有项目之间的确切大小顺序。此外，对于大小为C的缓存，所有访问频率大于等于1 / C的项都属于该缓存(在合理的假设下，被访问项的总数大于C)。因此，对于给定的样本量W，我们可以安全地用W / C限制计数器的上限。 如样本量16K，缓存大小2K，则计数器只需3位。
3. 作用：能够显著降低计数器的位数，进而减少Sketch的额外空间消耗。
Small Counters:
![img.png](/pic/tinyLFU/small_counters.png)
>思考：为什么可以这样简单的将值分为w/c份？   
> 假设 样本大小为16K(W) 缓存大小2K(C) 时间窗口W(自定)  
> 观点1:所有访问频率 >= 1 / C的项都属于该缓存。   
> 观点2:假设我们在某个时间段内有对M个不同对象的N个请求，对某一对象请求次数为D(D<=N)。则D/N >= 1/C 的时候，对象应该被缓存。   
> 观点3:N是可以改变的，也可以看成是窗口大小W(计数为W时自动reset)，因此 D/W >= 1/C 的时候，对象应该被缓存。我们并不在意D的值大小，而是相比的频率。进一步地看：W乘过去，D>=W/C的时候，对象应该被缓存。而D<W/C时，则需要进一步判断是否将对象加入到缓存中。   
> 观点4:承接上述所有观点，W为16K，缓存大小为2K时，如果D(访问次数)>=8，一定要缓存。如果D<8，则对比两个元素对应的访问次数(CM-Sketch获取)，大的应该被缓存。因此计数器只需要3位即可。  
> 这篇文章很清晰:https://9vx.org/post/on-window-tinylfu/

#### 3.3.2  Doorkeeper
>减少计数器的个数

1. 背景：在许多工作负载中，尤其是在重尾流量(heavy-tailed workloads)的工作负载中，冷门的项目占所有访问的很大一部分。这种现象意味着，如果我们计算样本中每个项目出现的次数，则大多数计数器将分配给样本中不太可能出现多次的项目。
2. 改进方案：Doorkeeper 是放置在近似计数Sketch方案前面的常规Bloom过滤器。插入元素命令到达后，我们首先检查元素是否包含在 Doorkeeper 中。如果 Doorkeeper 中未包含该元素，则将该元素插入到Doorkeeper中，否则，将其插入到主体结构中。查询项目时，我们同时使用了 Doorkeeper和Count-Min Sketch。就是说，如果该元素包含在 Doorkeeper 中，则TinyLFU会将该元素的频率作为其在Count-Min Sketch中的评估加一。否则，TinyLFU仅返回来自Count-Min Sketch的估计。执行重置操作时，除了将Count-Min Sketch中的所有计数器减半外，我们还清除了 Doorkeeper。这样做使我们能够保持新鲜机制。不过清除 Doorkeeper 也会使每个元素的估计值降低一，这也会使截断误差增加一。
3. 作用：由于它限制了插入到Count-Min Sketch中的唯一项的数量， 因此它可以使Count-Min Sketch更小。特别是，大多数尾项仅分配了1位计数器(在 Doorkeeper 中)。因此，在许多偏斜的工作负载中，此优化可显着减少TinyLFU的内存消耗。
Doorkeeper: 
![img.png](/pic/tinyLFU/doorkeeper.png)
>思考：Doorkeeper带来了哪些新颖点？   
> 通过布隆过滤器能够很好的降低长尾流量对缓存的影响，即减轻缓存污染问题，

## 4.TinyLFU局限
在一些数目很少但突发访问量很大的场景下，TinyLFU将无法保存这类元素，因为它们无法在给定时间内积累到足够高的频率，由此引出TinyLFU的变种——W-TinyLFU。
## 5.W-TinyLFU
W-TinyLFU主要用来解决一些稀疏的突发访问元素，结合LFU和LRU，前者用来应对大多数场景，而LRU用来处理突发流量。  

W-TinyLFU：主要由LRU、TinyLFU、SLRU构成
### 5.1 SLRU
问题背景：在实际的cache替换场景中，有一个很重要的现象，即如果一个缓存行被短时间访问了超过两次，那么他极有可能在近期再次被访问。LRU算法并没有基于该现象进行优化，而常常由于某一个常用缓存行在短时间内恰好没有使用而被替换出去。  

SLRU结构：Segmented LRU算法就是用来解决上述问题的，他通过在缓存行上面新加一个位，将缓存分为两个段(segment)，分别称作试用段(probationary segment)和保护段(protected segment)，区分在短时间内仅访问过一次的项目与至少两次访问过的暂时流行项目，试用段和保护段的比例为2:8。  

SLRU淘汰策略：两段中均使用LRU算法进行缓存淘汰，新项目总是插入到试用段中。当再次访问试用段中的某个项目时，该项目将移入受保护的段中，如果试用段已满，则根据LRU策略插入该段会导致驱逐其中一个项目。同样，任何插入到受保护段的操作都会驱逐其LRU 受害者。但在这种情况下，受害者会被重新插入试用段，而不是被完全遗忘。 这样，到达受保护段的时间上流行的项将比不流行的项在缓存中保留的时间更长。 

![img.png](/pic/tinyLFU/slru_cache.png)
![img.png](/pic/tinyLFU/slru_cache_detail.png)

> 思考：为什么SLRU能解决此问题？   
> SLRU和LRU唯一的区别就是当一个不在缓存中的行进入缓存时，LRU算法将它放在了第0号位置，即Most Recently Used的位置，而SLRU算法将它放在了不那么高的一个位置，即保护段和试用段连接处的位置。这样SLRU算法就不会担心某一些特定代码段（比如短时间塞入了大量无用缓存行）会完全破坏缓存的有效性了。

### 5.2 整体结构
![img.png](/pic/tinyLFU/tinyLFU_overview_2.png)
Window Cache(LRU)作用：准入窗口，是一个较小的LRU队列，其容量只有缓存大小的1%，作用主要是保护一些新进入缓存的数据，给他们一定的成长时间来积累自己的使用频率，避免被快速淘汰掉，同时这个窗口也可以过滤掉一些突发流量。  

>LRU和Doorkeeper不冲突嘛？两者作用分别是什么？只保留一个可以嘛？  
> LRU：这篇论文Adaptive Software Cache Management ，自适应的软件缓存管理，它的基本思想是在主缓存段之前放置一个 LRU “窗口”，并使用爬山技术自适应地调整窗口大小以最大化命中率。   
> Doorkeeper：将新 key 放入 TinyLFU 中之前，使用布隆过滤器首先检查key是否之前已被查看过。仅当 key 在布隆过滤器中已经存在时，才将其插入 TinyLFU。这是为了避免长时间不被看到的长尾键污染 TinyLFU。

TinyLFU作用：使用极小空间估计数据访问频次，通过对比两数据出现频次判断是否将数据加入主缓存中。  

SLRU作用：作为缓存淘汰主策略，提高缓存命中率，降低缓存污染问题影响。  

## 6.TinyLFU应用场景及展望
TinyLFU结合了LFU与LRU的优点，在各种情况下都具有较良好的性能。通过使用此缓存策略，可以提高缓存命中率。在开源项目Caffeine中使用W-TinyLFU构建「现代的缓存」。  

>问题：Caffeine如何在多线程情况下，处理并发更新问题(读：记录频次 写：更新缓存)  
> Caffeine优化：将读写操作按序放到缓冲区中进行批处理操作。   
> 对于读操作：将每一个读到的数据放到RingBuffer中(环形定长数组，便于GC)，之后便直接返回结果，类似于数据库的WAL机制，异步线程并发读取 RingBuffer 数组，更新访问信息。读缓冲中的事件主要是为了优化驱逐策略的命中率，因此读缓冲中的事件完整程度允许一定程度的有损。   
> 对于写操作：写操作将数据放到阻塞队列中——多生产者单消费者，写缓冲并不允许数据的丢失，因此其必须实现为一个安全的队列。

## 7.源码
TinyLFU实现原理&Caffeine代码解读:https://zhuanlan.zhihu.com/p/339625839  

GitHub TinyLFU Go实现源码:https://github.com/dgryski/go-tinylfu/blob/fba88f4a7f91124e5cc36723834506b20b5bae80/cm4.go#L69  

GO语言LRU实现:https://github.com/hashicorp/golang-lru  

Caffeine——现代的缓存 Java:https://github.com/ben-manes/caffeine/wiki/Simulator  

## 8.参考
什么是缓存，缓存策略有哪些?:https://blog.csdn.net/qq_39283195/article/details/94328205  

布隆过滤器:https://blog.csdn.net/qq_34713831/article/details/107458316  

CBF的原理与简单实现:https://cloud.tencent.com/developer/article/1136056  

Count-Min Sketch 数学推导:  https://zhuanlan.zhihu.com/p/84688298

TinyLFU阅读笔记:  https://segmentfault.com/a/1190000016091569

TinyLFU 笔记(写的好): https://9vx.org/post/on-window-tinylfu/  


## 9.有趣的观点
观点：只需要知道缓存中已经存在的潜在缓存替换受害者(victim)是否比当前正在访问的项目更受欢迎。但是，无需确定高速缓存中所有元素之间的确切顺序。
>...a frequency histogram only needs to know whether a potential cache replacement victim that is already in the cache is more popular than the item currently being accessed. However, the frequency histogram need not determine the exact ordering between all items in the cache.